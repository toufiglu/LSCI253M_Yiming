---
title: "case_study_RT_accuracy"
author: ""
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, error=TRUE, cache = FALSE)
```

```{r libraries, include=FALSE}
library(tidyverse)
library(magrittr)
library(lme4)
```

## Experiment design

Describe the design of the experiment in your own words.

```{r load-data}
d.all <- read.csv(file = "../data/xie_data_full.csv") %>%
  select(PartOfExp, Trial, Filename, Word, Response, RT, WorkerId, Condition, List, Speaker, VisualProbeType, Error, CorrectResponse, Block, BaselineRT, ListOrder, ListID, Phase, subjMean_BlockRT, BaselineRT_raw) %>%
   filter(Condition %in% 
           c("Mandarin-accented English (same)",
             "Control (native accent)" 
         ))
```

## Data cleaning

### 1. Examine RT distribution

Examine the distribution of RT (subjMean_BlockRT) across subjects. Does it make sense?

```{r distribution-of-subj-wise-mean-RTs-before-exclusions, fig.cap="Distribution of subjects mean RTs by Block and Condition, prior to outlier exclusions.\\label{fig:distribution-of-subj-wise-mean-RTs-before-exclusions}"}
summary(d.all$subjMean_BlockRT)
# The median RT is around 1200 ms, and the mean around 1500 ms. There are some outliers who might have consistently used longer time. (The longest took 62 s on average for each trial)

hist_subjmean <- function(data) {
  plot <- data %>% 
    arrange(subjMean_BlockRT) %>% 
    ggplot(aes(x=subjMean_BlockRT)) +
    geom_histogram(binwidth = 100,  color="black", alpha = 0.7) +
    labs(title = "Histogram of Subject mean block RT", x = "Mean RT", y = "Frequency") +
    theme_minimal()
  return(plot)
}
 
  
rt_dist <- hist_subjmean(d.all)
rt_dist

d.all <- d.all %>% mutate(participant = as.integer(factor(WorkerId))) %>% arrange(participant)
```

## 2. Data exclusion

Describe the procedure you take to exclude outliers (subjects, trials, etc.).

### Exclusion by subject
Describe your exclusion criteria based on a subject's performance.

e.g., We want to identify and remove subjects who consistently registered slow response times because they did not perform the task faithfully (e.g., multi-tasking) or because their computer equipment did not provide reliable recording of RTs over the web. 


```{r outlier-exclusion-subject}
## ----------------------------------------
# identify *eligible* subjects 
quartiles <- quantile(d.all$subjMean_BlockRT, probs = c(0.25, 0.75))
Q3 <- quartiles[2]
iqr_value <- IQR(d.all$subjMean_BlockRT)
exclusion <- Q3 + 1.5 * iqr_value

d.excluded <- d.all %>% 
  filter(subjMean_BlockRT <= as.numeric(exclusion))

exclusion_list <- unique(d.excluded$WorkerId)

d.eligible <- d.all %>% 
  filter(!(WorkerId %in% exclusion_list))

rt_eligible_dist <- hist_subjmean(d.eligible)
rt_eligible_dist

# how many RT-based subject exclusions in total
num_all_workers <- n_distinct(d.all$WorkerId)
num_eligible_workers <- n_distinct(d.eligible$WorkerId)
num_excluded = num_all_workers-num_eligible_workers
num_excluded
# I excluded 23 participants from the list. As long as they have one block above the criteria, the participant will be excluded.

# how many RT-based subject exclusions per Condition
native_all <- d.all %>% filter(Condition=="Control (native accent)") 
all_native_number <- n_distinct(native_all$WorkerId)

accent_all <- d.all %>% filter(Condition=="Mandarin-accented English (same)")
all_accent_number <- n_distinct(accent_all$WorkerId)

native_eligible <- d.eligible %>% filter(Condition=="Control (native accent)") 
eligible_native_number <- n_distinct(native_eligible$WorkerId)

accent_eligible <- d.eligible %>% filter(Condition=="Mandarin-accented English (same)")
eligible_accent_number <- n_distinct(accent_eligible$WorkerId)

native_loss <- all_native_number-eligible_native_number
accent_loss <- all_accent_number-eligible_accent_number

# Native lost 10 participants, while non-native accent condition lost 13 participants.
```

Re-examine RT distribution after subject exclusion.

```{r RT-distribution-after-outlier-removal-step1, fig.cap="...\\label{fig:RT-distribution-after-outlier-removal-step1}" }

rt_eligible_dist <- d.eligible %>% 
  arrange(subjMean_BlockRT) %>% 
  ggplot(aes(x=subjMean_BlockRT)) +
  geom_histogram(binwidth = 20, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Histogram of Subject mean block RT", x = "Mean RT", y = "Frequency") +
  theme_minimal()

rt_eligible_dist
```

### Exclusion by trial with extreme RTs

The second step of outlier removal was to exclude trials with atypical RTs. Describe your exclusion criteria by trial and do a second round of exclusion.

Q: Did trial-wise outlier exclusion disproportionately affect any experimental Conditions?

More trials were excluded for the non-native accent condition than native accent condition, but the difference is not huge (10 trials).

```{r outlier-removal-step2, echo = FALSE}
d.subject.eligible <- d.eligible %>%
  group_by(WorkerId) %>% 
  mutate(mean_workerRT=mean(RT),
         sd_workerRT=sd(RT)) %>% 
  filter(RT<mean_workerRT+2*sd_workerRT) %>% 
  ungroup()

eligible_native_trials <- d.eligible %>% filter(Condition=="Control (native accent)") %>% n_distinct() 
eligible_accent_trials <- d.eligible %>% filter(Condition=="Mandarin-accented English (same)") %>% n_distinct() 

subject_native_trials <- d.subject.eligible %>% filter(Condition=="Control (native accent)") %>% n_distinct() 
subject_accent_trials <- d.subject.eligible %>% filter(Condition=="Mandarin-accented English (same)") %>% n_distinct()

native_subject_loss <- eligible_native_trials - subject_native_trials
accent_subject_loss <- eligible_accent_trials - subject_accent_trials

# Native accent lost 68 trials, while non-native accent lost 78 trials. The difference is not drastic.
# Compare the effect for all 6 blocks
trials_before <- d.eligible %>%
  group_by(Block) %>%
  summarise(num_trials = n(), .groups = 'drop')

trials_after <- d.subject.eligible %>%
  group_by(Block) %>%
  summarise(num_trials = n(), .groups = 'drop')

trials_comparison <- trials_before %>%
  left_join(trials_after, by = c("Block"), suffix = c("_before", "_after")) %>%
  mutate(loss = num_trials_before - num_trials_after)

trials_comparison

d.eligible <- d.eligible %>% mutate(exclusion="before")
d.subject.eligible <- d.subject.eligible %>% mutate(exclusion="after") %>% dplyr::select(-c(mean_workerRT, sd_workerRT))

d.plot <- rbind(d.eligible, d.subject.eligible) %>% mutate(exclusion=factor(exclusion, levels=c("before", "after")))
dist_block <- d.plot %>% 
  ggplot(aes(x=RT)) +
  facet_grid(exclusion ~ Block) +
  geom_histogram(binwidth = 20, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Histogram of RT", x = "Mean RT", y = "Frequency") +
  theme(axis.text.x = element_text(angle=90, hjust=1, size = 5))

dist_block

```

Q: Examine the mean RTs by block. Do they vary a lot before and after trial exclusion? Describe the effects.
Around 11-16 trials were excluded from each block other than the exercise block. Since the exercise block is not counted towards the conclusion, there does not seem to be a disproportionate impact based on block. As is shown in the histogram above, it seems that only the outlier trials were excluded, leaving the general pattern untouched.

## Examine RTs and Accuracy during practice and baseline (after exclusion steps 1 and 2)

Now that we've excluded extreme subject and trial outliers, we can look at the practice and baseline data to assess our high-level predictions about how participants should perform on this web-based task.

1. **One data pattern that we expect to find is that performance (both RTs and accuracy) in the practice and baseline blocks is comparable across experimental conditions.** We expect this because these blocks of the experiment were identical across conditions (i.e., native-accented stimuli presented in the clear).
    
    + ... *if performance in the **practice block** differs substantially across conditions*, we would need to consider whether the subjects in each condition were sampled from the same underlying population (e.g., did we run all conditions at approximately the sme time of day?).

    + ... *if performance in the **baseline block** differs substantially across conditions*, we would need to consider whether exposure to different types of speech during the main block of the experiment induced overall differences in task performance (in which case the baseline block doesn't provide a reliable condition-independent "baseline" for normalization purposes).

2. **A second data pattern that we expect to find is evidence of improvement (adaptation) over the course of the task.** One way this would manifest is faster RTs and increased accuracy in the post-experiment baseline block, relative to the practice phase. 


## Summary of exclusion criteria:\label{sec:summary-of-exclusion-criteria}

- Participant-level exclusions:
    + 1.5 interquartile range above 75% of subject median RT
    + If a participant has one block that is beyond the threshold, their entire data were excluded
    + More than 20 participants were excluded as a result
    
    
- Trial-level exclusions:
    + 2 standard deviation above each participant's RT
    + 

We applied the same exclusion criteria across all RT and error analyses.

## Normalize experimental RTs relative to baseline

Now that we've completed all trial-wise RT exclusions, we can calculate _normalized_ RTs that take into account each subject's baseline speed on this task. For this procedure, we adjust the RTs on each trial by subtracting out the corresponding subject's mean RT during the baseline phase. We refer to the resulting measure as _adjusted RTs_.

```{r, echo = TRUE}
# calculate each subject's mean Baseline RT
# and subtract that value from experimental RTs
d.subject.eligible %<>%
  group_by(WorkerId) %>%
  mutate(
    # calculate subject-wise mean RTs during baseline block
    meanBaselineRT = mean(RT[PartOfExp == "baseline"]),
    
    # calculate normalized RTs
    AdjustedRT = RT - meanBaselineRT,
    
    # calculate subject-wise mean Adjusted RT across Blocks 1-4
    meanAdjustedRT = mean(AdjustedRT[PartOfExp == "main"])
  ) 

rt_adjusted <- d.subject.eligible %>% 
  ggplot(aes(x=AdjustedRT)) +
  facet_grid(~ Block) +
  geom_histogram(binwidth = 20, color = "black", alpha = 0.7) +
  labs(title = "Histogram of RT", x = "Talker normalized RT", y = "Frequency") +
  theme(axis.text.x = element_text(angle=90, hjust=1, size = 5))

rt_adjusted
```
It seems that the data distribution is less skewed after normalization?

Now we want to check the distribution of adjuted RTs to make sure it seems reasonable, given our expectations about task performance.

Note that we expect baseline RTs to be faster on average than RTs during the experimental block, regardless of exposure condition. We expect this for two reasons. First, the baseline task occurred at the end of the experiment, after participants had adapted to the task. Second, _all_ participants heard native accented speech during the baseline phase; hence, there was no need for accent adaptation during this phase.


# Modeling strategy

## Model building and assessment
RTs were analyzed using linear mixed effects regression, as implemented in the lme4 package (version 1.1-10: Bates, Maechler, Bolker, \\& Walker, 2014) in R (R Core Team, 2014). Response accuracy (incorrect vs. correct response) was analyzed using mixed effects logistic regression (see Jaeger, 2008). All mixed effects models were specified with the maximal random effects structure justified by the experimental design: that is, by-subject and by-item random intercepts, by-subject random slopes for all design variables manipulated within subjects, and by-item random slopes for all design variables manipulated within items. If the definitionally maximal model failed to converge within ten thousand iterations, the model was systematically simplified in a step-wise fashion until the model converged. These steps involved removing correlations among random effects; dropping the random effects term with the least variance; and removing fixed effects that were inconsequential for the theory being tested (i.e., counterbalancing nuisance variables).

## Variable coding
Unless otherwise specified, all numeric predictors were centered and categorical predictors were coded as sum contrasts, in order to reduce collinearity among predictors. 

```{r prep-lmer}

# change to dat_out3 to implement 3rd outlier step
dat <- d.subject.eligible %>%
  filter(PartOfExp == "main") %>%
  filter(!(Block == "practice")) %>% 
  droplevels(.)

## ------------------------------------------ 
## Define contrast coding for analyses
## ------------------------------------------ 

dat <- within(dat %>% mutate(Block = factor(Block)), {
  # helmert coding for Block for C&G-style analysis
  contrasts(Block) <- contr.helmert(4)
})

## ------------------------------------------ 
## EXPERIMENT 1
exp1 <- dat %>%
  within(., {
  # sum coding for accent condition
  Condition <- factor(Condition)
	contrasts(Condition) <- cbind("Accented" = c(1,-1))
	
	 # sum contrast code List (counterbalancing nuisance factor)
	List <- factor(List)
  contrasts(List) <- contr.sum(nlevels(List))
  colnames(contrasts(List)) <- rownames(contrasts(List))[1:7]
  
  # sum code ListID
  ListID <- factor(ListID)
  contrasts(ListID) <- contr.sum(nlevels(ListID))

  #sum code ListOrder
  ListOrder <- factor(ListOrder)
  contrasts(ListOrder) <- contr.sum(nlevels(ListOrder))
})
```


# Experiment 1: Adaptation to Mandarin-accented English
## Participants

Examine the number of participants per condition.

```{r examine-number-of-participants}
unique_worker <- exp1 %>%
  group_by(Condition) %>%
  summarise(num_unique_workers = n_distinct(WorkerId), .groups = 'drop')

unique_worker
# There are 39 participants for the native accent condition and 38 for the non-native condition.
```
## Exp1 Response Times

Visualize the changes of RTs across blocks by condition.

```{r exp1-RTs-by-condition, fig.width = 11, fig.height = 5, fig.cap="Average RTs by exposure condition in Experiment 1.\\label{fig:exp1-RTs-by-condition}"}
d.RT <- exp1 %>% 
  group_by(Block) %>% 
  mutate(meanRT=mean(AdjustedRT), sdRT=sd(AdjustedRT), lowerRT=meanRT-2*sdRT, upperRT=meanRT+2*sdRT) %>% 
  ungroup()

trajectory_RT <- d.RT %>% 
  ggplot(aes(x=Block, y=meanRT, group = 1)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin = lowerRT, ymax = upperRT), width = 0.2)

trajectory_RT
# There seems to be a small decrease in the mean RT across the five condition, but in view of the error bar, this difference does not seem to be significant.
```

We assess the effect of exposure condition (Mandarin-accented English vs. control) on processing speed separately for RTs during the exposure phase and the test phase. To assess the _change_ in RTs during the course of exposure, we split the 18-trial exposure phase into three blocks of 6 trials and use the resulting Block variable as a categorical predictor of RTs. We use linear mixed-effects models to simultaneously model subject and item random effects.


#### Exposure
A linear mixed effects model was fit to adjusted RTs for correct responses during the exposure phase. 

Describe your fixed effects and random effects. Describe how each variable is coded.

The fixed effects are condition, block, as well as their interactions. The random effects are the list, listID, listOrder, word and participant. Each random effect considered both intercepts and slopes. Helmert coding was used for block, while the other variables all used sum coding. t-tests shows that a significant main effect of block 2, meaning that there was a reduction of RT by 114.7 ms during the second chunk. But going forward, we do not see a further improvement in RT in the third block. There is a marginal effect of accent nativeness, and no interaction between nativeness of the accent and block. 

```{r exp1-byBlock-exposureRT, echo = TRUE}
# Model specification:
# by-block analysis of RTs during EXPOSURE
d.RT.exposure <- exp1 %>% filter(Phase=="Exposure phase")
m.RT.exposure <- lmer(AdjustedRT ~ Block * Condition + (Block * Condition|ListID) + (Block * Condition|List) + (Block * Condition|ListOrder) + (Block * Condition|WorkerId) + (Block * Condition|Word), data= d.RT.exposure)

summary(m.RT.exposure)
```


#### Test
I don't think it will work here though, because test phase is block 4, so there is only one level here.
```{r exp1-byBlock-testRT, echo = TRUE}
# Model specification:
# by-block analysis of RTs during TEST
d.RT.test <- exp1 %>% filter(Phase=="Test phase")
m.RT.test <- lmer(AdjustedRT ~ Condition + 
                  (1 + Condition | ListID) + 
                  (1 + Condition | List) + 
                  (1 + Condition | ListOrder) +
                  (1 + Condition | WorkerId) + 
                  (1 + Condition | Word), 
                  data = d.RT.test)

summary(m.RT.test)
```

Since there is only one block of tests, I cannot use block as a main effect. The above model returns no significant main effect of condition, suggesting that listeners' RT do not differ depending on different accent conditions.